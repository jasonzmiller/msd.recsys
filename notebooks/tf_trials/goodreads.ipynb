{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a77ef772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cb0ba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv('datasets/goodreads/ratings.csv')\n",
    "books_df = pd.read_csv('datasets/goodreads/books.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "097a32e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(981756, 3)\n",
      "53424\n",
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "book_id    0\n",
       "user_id    0\n",
       "rating     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ratings_df.shape)\n",
    "print(ratings_df.user_id.nunique())\n",
    "print(ratings_df.book_id.nunique())\n",
    "ratings_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "665047fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data: (785404, 3)\n",
      "Shape of test data: (196352, 3)\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest = train_test_split(ratings_df, test_size=0.2, random_state=1)\n",
    "print(f\"Shape of train data: {Xtrain.shape}\")\n",
    "print(f\"Shape of test data: {Xtest.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdf6e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the number of unique entities in books and users columns\n",
    "nbook_id = ratings_df.book_id.nunique()\n",
    "nuser_id = ratings_df.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5819e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Book input network\n",
    "input_books = tf.keras.layers.Input(shape=[1])\n",
    "embed_books = tf.keras.layers.Embedding(nbook_id + 1,15)(input_books)\n",
    "books_out = tf.keras.layers.Flatten()(embed_books)\n",
    "\n",
    "#user input network\n",
    "input_users = tf.keras.layers.Input(shape=[1])\n",
    "embed_users = tf.keras.layers.Embedding(nuser_id + 1,15)(input_users)\n",
    "users_out = tf.keras.layers.Flatten()(embed_users)\n",
    "\n",
    "conc_layer = tf.keras.layers.Concatenate()([books_out, users_out])\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(conc_layer)\n",
    "x_out = x = tf.keras.layers.Dense(1, activation='relu')(x)\n",
    "model = tf.keras.Model([input_books, input_users], x_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f28e9",
   "metadata": {},
   "source": [
    "First 3 lines: create input layer to accept a 1D array of book IDs, create an embedding layer with shape of (num unique books + 1, 15). We add 1 to the number of unique books because the embedding layers need an extra row for books that do not appear in the training dataset, OOV entitites. The second dimension (15) is arbitrary. This can be any number depending on how large we want the embedding layer to be.\n",
    "\n",
    "Notice that we append the input layer to the end of the book embedding layer. This is the functional API in action. What we are basically saying here is that we want to pass the output of the input layer to the embedding layer.\n",
    "\n",
    "In the next three lines of code, we do the same thing we did for books, but this time for the users. That is, we create an input that accepts the users as a 1D vector, and then we create the user embeddings, as well.\n",
    "\n",
    "In the concatenate line, we simply concatenate or join both the books and the user embedding layer together, and then add a single dense layer with 128 nodes on top of it. For the final layer of the network, we use a single node, because weâ€™re predicting the ratings given to each book, and that requires just a single node.\n",
    "\n",
    "In the last line of code, we use the `tf.keras.Model` class to create a single model from our defined architecture. This model is expecting two input arrays (books and users)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d716846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 15)        150015      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 15)        801375      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 15)           0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 15)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 30)           0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          3968        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            129         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 955,487\n",
      "Trainable params: 955,487\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = tf.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60346fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12272/12272 [==============================] - 74s 6ms/step - loss: 0.8561 - val_loss: 0.7190\n",
      "Epoch 2/5\n",
      "12272/12272 [==============================] - 78s 6ms/step - loss: 0.6849 - val_loss: 0.6976\n",
      "Epoch 3/5\n",
      "12272/12272 [==============================] - 84s 7ms/step - loss: 0.6485 - val_loss: 0.6934\n",
      "Epoch 4/5\n",
      "12272/12272 [==============================] - 73s 6ms/step - loss: 0.6172 - val_loss: 0.6961\n",
      "Epoch 5/5\n",
      "12272/12272 [==============================] - 73s 6ms/step - loss: 0.5860 - val_loss: 0.7156\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([Xtrain.book_id, Xtrain.user_id], \n",
    "                 Xtrain.rating, \n",
    "                 batch_size=64, \n",
    "                 epochs=5, \n",
    "                 verbose=1, \n",
    "                 validation_data=([Xtest.book_id, Xtest.user_id], Xtest.rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8ce31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
